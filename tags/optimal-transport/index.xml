<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Optimal Transport on optstats</title>
    <link>https://optstats.github.io/tags/optimal-transport/</link>
    <description>Recent content in Optimal Transport on optstats</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© Copyright notice</copyright>
    <lastBuildDate>Fri, 02 Sep 2022 04:56:58 +0530</lastBuildDate><atom:link href="https://optstats.github.io/tags/optimal-transport/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Lecture 1 Introduction to Optimal Transport</title>
      <link>https://optstats.github.io/2022/09/lecture-1-introduction-to-optimal-transport/</link>
      <pubDate>Fri, 02 Sep 2022 04:56:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/09/lecture-1-introduction-to-optimal-transport/</guid>
      <description>Monge Problem 1. Background To transport a pile of sand into some holes, such as from point $x$ to $T(x)$; We concern which in an efficient way to transport formulated with cost $x-T(x)$; The over all transport formulated by $\int [x-T(x)]f(x)dx$; We could understand the formula above in this way, the $x-T(x)$ denotes the unit mass transport cost, the $f(x)$ denotes the mass on opint x, so the whole transport cost on point $x$ is $[x-T(x)]f(x)$.</description>
    </item>
    
    <item>
      <title>Lecture 16 Gradient Flow</title>
      <link>https://optstats.github.io/2022/09/lecture-16-gradient-flow/</link>
      <pubDate>Thu, 01 Sep 2022 04:01:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/09/lecture-16-gradient-flow/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Lecture 15 Wasserstein Curves</title>
      <link>https://optstats.github.io/2022/09/lecture-15-wasserstein-curves/</link>
      <pubDate>Thu, 01 Sep 2022 02:01:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/09/lecture-15-wasserstein-curves/</guid>
      <description> 对于W-空间两个分布$u_0$和$u_1$的两个csg，$u_s$和$u_t$： $$W_p(u_s,u_t)\leq |t-s|W_p(u_0,u_1)$$
Curves in $W_p$ 考虑$u_t,t\in (0,1)$是$W_p(X)$空间的一个连续曲线，其中$p&amp;gt;1$，$X$是凸的支撑集，则存在一个向量场$v_t$，曲线沿着向量场流动生成 $$\partial_t u_t+\nabla(v_tu_t)=0$$
关于csg的思考 为什么说是考虑成对测度：因为csg是基于最优耦合去构造的，需要一对分布去得到csg； 如果将csg看做一个连续曲线，则可以将其动态生成（有t）与向量场关联，每个$u_t$的梯度是下一个方向，也是最优下降方向即$u_t^{&amp;rsquo;}=-\delta \nabla u_{t-1}$； 曲线生成过程是一个用离散采样逼近连续的过程，将最优耦合看作一个状态转移映射$f$， $$(u_t,u_{t-1})\sim \pi^*_{t,t-1}$$
$$u_t\sim f(u_{t-1})$$
$$min_f E_{\pi^*} ||u_t-f(u_{t-1})||^2$$
梯度和梯度流基础
离散的时间$k\in Z_0\lbrace0,1,2,,,\rbrace$，连续时间$t\in R_0=[0,\infty)$，离散时间可以看作是连续时间的一个离散化，基于一个无穷小的步，$t=\delta k,\ \delta&amp;gt;0$； 一个空间上的动力学是一个随着时间变化的状态曲线； 状态曲线是如何随着时间生成或者变化的？ 状态曲线是连续时间场景的梯度流生成，但我们用离散时间的梯度下降算法去近似连续时间的梯度流微分方程的光滑解； 状态转移方向：向量场方向，$dX_{t}=F(X_t)=-\nabla X_t$，其中$F$是向量场，表示下一刻向哪里运动，也即向最速下降方向； 连续两个离散时间点的转移关系：注意连续时间没有下一刻的概念，这里用一种线性近似去拟合,因此有$X_{t+\delta}=X_t+\delta F(X_t)$； 如果从$t=0$开始，则生成一个$X_t(X_{\delta t})$序列且满足$X_{t+1}=X_t+\delta F(X_t)$，注意梯度下降的步长和时间变化步长作用类似，所以用相同 $\delta$ 表示； 梯度流到底在做什么数学问题：连续时间的凸优化问题的求解，按照最速下降的方向去优化目标； 梯度流和梯度下降可以看出：在离散和连续两个赛道的优化，可以并发处理且关联上，且保持相同的分析结果，如收敛分析，但离散情形要做一些假定，如函数是$L$-平滑等； 负梯度方向对应的优化问题形式 $$-\nabla f(x)=argmin_{v\in R^d}\lbrace&amp;lt;\nabla f(x),v&amp;gt;+\frac{1}{2}||v||^2\rbrace$$
Reference Gradient flow and gradient descent </description>
    </item>
    
    <item>
      <title>Lecture 13 Scaling Algorithm for Entropic OT</title>
      <link>https://optstats.github.io/2022/08/lecture-13-scaling-algorithm-for-entropic-ot/</link>
      <pubDate>Wed, 31 Aug 2022 12:29:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/08/lecture-13-scaling-algorithm-for-entropic-ot/</guid>
      <description>Sinkhorn&amp;rsquo;s Algorithm for Entropic Optimal Transport 对于熵正则最优传输 $$min_{\pi}\sum_{ij} \pi_{ij}c_{ij}+\epsilon\sum_{ij}\pi_{ij}log\pi_{ij}$$ $$s.t.\ \mathbf{\pi}\mathcal{1}=a$$ $$\mathbf{\pi}^T\mathcal{1}=b$$
其最优解唯一，形式为$\pi^\ast_{ij}=u_iK_{ij}v_j$，其中$K_{ij}=exp(c_{ij})\over\epsilon$，且最优传输矩阵
$$\pi^\ast=Diag(u)KDiag(v)$$
熵正则最优解形式推导 写出原问题的拉格朗日函数 $$L(\pi,u,v))\sum_{ij} \pi_{ij}c_{ij}+\epsilon\sum_{ij} \pi_{ij}log \pi_{ij}-u(\pi\mathbf{1}-a)-v(\pi^T\mathbf{1}-b)-\epsilon(\sum_{ij} \pi_{ij}-1)$$
处于方便计算，上式体现传输矩阵有个正则化约束项，可改写熵正则的目标问题等价$\epsilon\sum\pi log\pi-1$，基于此，可以理解为什么拉格朗日惩罚系数是$\epsilon$；
对Larg函数求最优点，注意$\pi_{ij}$求导，直接看线性结构的系数（向量分量）；
$$\frac{\partial L}{\partial\pi_{ij}}=c_{ij}+\epsilon(log\pi_{ij}-1)-u_i-v_j-\epsilon=c_{ij}+\epsilon log\pi_{ij}-u_i-v_j=0$$
$$log\pi^\ast_{ij}=\frac{-c_{ij}+u_i+v_j}{\epsilon}$$
$$\pi^\ast_{ij}=exp(\frac{-c_{ij}+u_i+v_j}{\epsilon})=exp(\frac{u_i}{\epsilon})exp(-\frac{c_{ij}}{\epsilon})exp(\frac{v_j}{\epsilon})$$
Scaling Algorithm
我们要想求解最优传输矩阵$\pi$，通过上面正则解的推导，得到最优矩阵的元素$K_{ij}$与$u_i,v_j,c_{ij},\epsilon$有关；因此，如果恢复矩阵$\pi$，我们关键需要求解向量$u$和$v$，即产生以下约束问题： $$u= [exp(\frac{u_i}{\epsilon})]$$ $$v= [exp(\frac{v_j}{\epsilon})]$$ $$s.t.\ Diag(u)KDiag(v)\mathcal{1}=a $$ $$Diag(v)K^TDiag(u)\mathcal{1}=b $$
理解上述式子需要注意：
$K$矩阵元素与代价矩阵元素有关； 对角阵左乘一个矩阵，等价于矩阵的每行乘对角阵对应元素； 对角阵右乘一个矩阵，等价于矩阵的每列乘对角阵对应元素； $Diag(u)KDiag(v)=\pi$，对于传输矩阵$\pi$的元素$\pi_{ij}=u(i)K_{ij}v(j)$，即$\pi_{ij}$由三部分构成，可以考虑计算结合过程为$K_{ij}$的$i$对应着$u(i)$，$K_{ij}$的$j$对应着$v(j)$； 第四个式子可以看成第三个式子的转置，结合传输矩阵转置与单位列向量乘即对逐列求和可得； 结合以上，我们知道等价于解关于$u$和$v$的方程组： $$Diag(u)Kv=a$$ $$Diag(v)K^Tu=b$$
对此，交替迭代的Sinkhorn算法如下：
初始化$v^0=(1,1,1)^T$； 基于第一个方程更新$u^1$； 基于第二个方程更新$v^1$； 在$l+1$轮，用$v^l$更新$u^l$，根据$u^{l+1}$来更新$v^{l+1}$，得到$(v^{l+1},u^{l+1})$，从而得到相应的$\pi$阵；
算法的几何解释：
在两个空间交替投影（找到适合问题的解），一个空间是行和为$a$的矩阵空间，一个空间为行和为$b$的矩阵，算法最终收敛在两个空间相交的地方，即表示行和为$a$且列和为$b$的传输矩阵； 算法理论基础：
两个闭凸集$C$和$D$，假设在这两个闭凸集上的投影算子$P_C$和$P_D$，当进行迭代交替的投影时，最终两个投影输出会落在$C\cap D$上； 更多解释可参考Note； </description>
    </item>
    
    <item>
      <title>Lecture 12 Entropy and Second Law</title>
      <link>https://optstats.github.io/2022/08/lecture-12-entropy-and-second-law/</link>
      <pubDate>Wed, 31 Aug 2022 10:20:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/08/lecture-12-entropy-and-second-law/</guid>
      <description>Entropy Regularized Optimal Transport 香农熵：一个分布的香农熵在有限空间$\sum_{i=1}^k p_i=1$为 $$-\sum_i p_ilogp_i$$
分布越均匀，熵越大，均匀分布的熵最大，分布越偏，熵越小； 对于两个分布的耦合$\pi$，其最优传输问题为： $$min_{\pi}\sum_{ij} \pi_{ij}c_{ij}$$ $$s.t.\ \mathbf{\pi}\mathcal{1}=r$$ $$\mathbf{\pi}^T\mathcal{1}=c$$
对应的熵正则最优传输为： $$min_{\pi}\sum_{ij} \pi_{ij}c_{ij}+\epsilon\sum_{ij} \pi_{ij}log \pi_{ij}$$ $$s.t.\ \mathbf{\pi}\mathcal{1}=r$$ $$\mathbf{\pi}^T\mathcal{1}=c$$
原始的离散最优传输问题是一个线性规划问题，其最优解在约束凸集的顶点处取得； 分析带有熵正则的最优传输，对于一个耦合，为什么顶点处的熵是零； 如何分析熵正则的最优传输将最优点从边界顶点处推向约束凸集的内部； </description>
    </item>
    
    <item>
      <title>Lecture 14 Wasserstein Space</title>
      <link>https://optstats.github.io/2022/08/lecture-14-wasserstein-space/</link>
      <pubDate>Tue, 30 Aug 2022 02:01:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/08/lecture-14-wasserstein-space/</guid>
      <description> 度量：是一个函数，定义为一个距离，满足以下，
对称$d(x,y)=d(y,x)$； 非负仅当$d(x,x)=0$； 三角不等式 Wasserstein 空间$(X,D)$：参考度量空间的概念，可以理解为在样本空间$X$上，定义度量D为Wasserstein距离；
n维空间上的度量：$L$-$p$范数，如2-范数，无穷范数等，有的范数基于内积定义，内积的好处在于可以定义距离和角度等一系列测度；
分布的距离 Total variation distance：相同支撑维度，对应支撑测度差的绝对值和；
Hellinger distance：2-范数的一半；
上面两个距离无法区分下图分布之间距离，分析为什么； 最优传输距离：通过最优传输来定义两个分布$a,b$的距离为$W_p(a,b)$，称为$p$-wasserstein距离，$p\geq1$，一般取$p=2$；
最优传输研究两个测度之间的距离，这种问题的定义形式构成一个W-dis； W-dis是一个度量 构造耦合的技巧值得参考，结合下图 考虑三个分布$a,b,c$即$\sum a_i=\sum b_i=\sum c_i=1$，定义一个耦合$S=\pi^\ast diag{\frac{1}{\tilde{b}}}\gamma^\ast$，也就是要检查这个耦合的两个边缘测度是分布$a$和分布$c$； 曲线 找两个分布的中介曲线，如下图，如果直接是分布的凸组合，得到的更像一个混合模型； 借用优化的观点来思考这个问题，如求二维空间两个点$x,y\in R^2$的中点$z=\frac{x+y}{2}$，优化模型为 $$z=argmin_z ||z-x||^2+||z-y||^2$$ 通过梯度$dz$可以得到$z=\frac{x+y}{2}$；
Barycenter：距离多个分布平均W-距离最小的分布； $$b^{\ast}=argmin_{b}\sum_{i=1}^n \theta_i W_p^p(b,a_i),\ \sum\theta_i=1$$ 可见所求的barycenter是在多个分布构成的凸集内；
constant speed geodesic(csg)
定义：给定两个分布$a_0$和$a_1$，其csg为一个分布族$\lbrace a_t\rbrace_{t\in(0,1)}$，且满足$$W_p(a_0,a_t)=tW_p(a_0,a_1)$$ 如何构造得到$a_t$：从$a_0$和$a_1$的最优耦合$\pi^\ast$推导； 对于有限样本支撑空间$\mathcal{X}=\lbrace x_1,x_2,,,x_n\rbrace$， $$P_t:\ \mathcal{X}*\mathcal{X}\rightarrow \mathcal{X}$$ $$(x,y)\rightarrow tx+(1-t)y$$ 定义$a_t=\mathbf{E}_{\pi^\ast} P_t(X,Y),\ X\sim a_0,\ Y\sim a_1$ $$a_t=\sum_{ij}^n\pi_{ij} P_t(X,Y), \ (X,Y)\sim \pi^{\ast}$$
$$W_p(a_0,a_t)=tW_p(a_0,a_1)$$
$$W_p(a_{t1},a_{t2})=(t_2-t_1)W_p(a_0,a_1)$$
对csg的构造分析 csg分布是基于两个边缘分布及其最优耦合构造得到； csg是指使$W_p$满足线性关系的分布族； $P_t$是产生两个分布的凸组合分布，这个分布位于两个端点分布之间； </description>
    </item>
    
  </channel>
</rss>
