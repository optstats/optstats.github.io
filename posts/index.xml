<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on optstats</title>
    <link>https://optstats.github.io/posts/</link>
    <description>Recent content in Posts on optstats</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>©Copyright Hugo</copyright>
    <lastBuildDate>Fri, 23 Sep 2022 10:23:58 +0530</lastBuildDate><atom:link href="https://optstats.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Lecture 5 Duality Theory</title>
      <link>https://optstats.github.io/2022/09/lecture-5-duality-theory/</link>
      <pubDate>Fri, 23 Sep 2022 10:23:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/09/lecture-5-duality-theory/</guid>
      <description>5.1 拉格朗日对偶函数 拉格朗日函数与拉格朗日对偶函数：后者是拉格朗日函数的最小值 为什么用对偶： 原问题不好解（非凸、NP-hard等），通过对偶问题得到原问题的解（某些条件，解等价时）或解信息（如原问题解的下界） 探究原问题解的存在性等理论性质 转换成对偶问题，探究问题之间的等价联系 不等式约束的乘子$\lambda$是负的，导致是拉格朗日函数是小于等于原函数的 对偶函数是原问题最优值的一个下界（注意不是下确界）：对偶函数小于等于原函数任一可行解处的函数值，也就小于最优可行解的函数值 5.2 拉格朗日对偶问题 找原问题的最优值可能是困难的，转而找对偶函数，即找原问题最优值的一个下界 拉格朗日对偶问题：如何找最好的一个下界，即越接近原问题最优值的下界，应该是最大的下界，所以问题转化为极大化对偶函数 $$max\ d(\lambda,\mu) \ s.t. \lambda\geq 0$$ 最优拉格朗日乘子：一组$\lambda^\ast,\mu^\ast$是拉格朗日对偶问题的最优解 无论原问题的凸性，拉格朗日对偶问题总是一个凸优化问题（求解过程中，注意原函数的和对偶函数的凸性） 为什么拉格朗日对偶函数是一个凹问题，关于$\lambda$和$\mu$的线性规划逐点求最小值为啥是一个凹的问题？ 拉格朗日对偶问题是一个$max$-$min$问题 如果交换$max$-$min$的顺序，我们发现拉格朗日函数的先$max$后$min$就是原问题 对偶变量：一般是$\lambda$，也即对偶问题是关于参数的函数 有时候无法显示的用对偶变量表示原变量，这时我们可以将最优条件公式直接作为对偶问题的约束条件 5.3 从零和博弈看对偶问题 零和博弈（$F(x,y)$表示$P$给$H$的）的双方一个$P$是$min$-$max$，即先$max$，最坏情况，支付出去的，$\sum_yF(x,y)$；一个H是$max$-$min$，即先$min$，最坏情况，收入最少的$\sum_xF(x,y)$；双方是对偶的（可以联系线性规划的对偶问题，两个角度由同一个问题关联纠缠） 5.4 对偶定理 弱对偶定理：$d(\bar\lambda,\bar\mu)\leq f(\bar x)$，这里括号内分别是原问题和对偶问题的可行解 原问题的最优值一定大于等于对偶问题的最优值，即$max\ d(\lambda,\mu)\leq min \ f(x)$ 对偶间隙：$v(P)-v(D)$，原问题最小值和对偶问题最大值之间的差 线性规划，对偶间隙为0，一般非线性规划，不为0 </description>
    </item>
    
    <item>
      <title>Lecture 4 Constrained Optimization</title>
      <link>https://optstats.github.io/2022/09/lecture-4-constrained-optimization/</link>
      <pubDate>Fri, 23 Sep 2022 10:22:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/09/lecture-4-constrained-optimization/</guid>
      <description>4.1 约束优化概述 两个可以形式化为约束优化问题的案例：低秩矩阵补全和低秩矩阵恢复
矩阵补全：
通过已知元素，来补全矩阵的未知元素 矩阵补全往往要求补全矩阵的秩最小（目标函数），因为现实大规模数据矩阵一般是低秩的； 低秩矩阵：$rank(X)\leq m$，$rank(X)\leq n$； 数学模型：$$ min\ rank(X)$$ $$s.t. X_{ij}=M_{ij} \ i,j\in \Omega$$ 其中，$M$表示原矩阵，$\Omega$表示已知元素下标的集合。 上式目标函数是一个非凸函数，无法保证全局最优，且计算复杂，是一个NP难问题，研究论文有指出用一个矩阵核范数（奇异值和）近似秩，得到一个如下凸松弛： $$ min\ ||X||_\ast$$
$$s.t.\ X_{ij}=M_{ij} \ i,j\in \Omega$$
假定矩阵观测值是含有噪声的$Y=M+Z$，则有研究如下模型：
$$ min\ ||X||_{\ast}$$
$$s.t.\ ||P_{\Omega}(X-Y)_F||\leq \delta$$
低秩矩阵恢复： 清晰的图片往往是低秩的，有噪声存在，往往会提高秩； 秩在图片中表示包含的信息丰富程度； 数学模型：观察M=低秩X+稀疏噪声S $$min_{X,S\in R^{m\ast n}} \ rank(X)+\lambda||S||_0$$ $$s.t.\ X+S=M$$ 由于目标函数两部分都是非凸（$l_0$范数和$rank$），所以常用凸松弛后的模型RPCA|LRSMD如下： $$min_{X,S\in R^{m\ast n}} \ ||X||_*+\lambda||S||_1$$
$$s.t.\ X+S=M$$
4.2 约束优化的一阶最优性条件1 无约束最优性条件主要跟目标函数有关，约束优化最优性条件还跟约束函数有关； 有效约束$g(x^\ast)=0$与非有效约束$g(x^\ast)&amp;lt;0$，其中$x^\ast$是可行域中点； 如何理解有效约束？
有效约束指标集：可行解满足等式约束的相应等式指标、可行解满足不等式约束取等时（有效约束）的相应等式指标； 对于一个可行点，可以谈论其有效指标集是什么； 方向
可行方向：$x^\ast \in\Omega,0\neq \mathbf{d}\in R^n$，如果$\exists \delta&amp;gt;0,\ \forall t\in[0,\delta]$，使$(x^\ast +t\mathbf{d})\in\Omega$，则称$d$为$x^\ast$处的一个可行方向；
可行方向不唯一，沿可行方向走可仍然在可行域内，一个可行点处所有可行方向构成一个集合$FD(x^*)$；</description>
    </item>
    
    <item>
      <title>Lecture 3 Unconstrained Optimization</title>
      <link>https://optstats.github.io/2022/09/lecture-3-unconstrained-optimization/</link>
      <pubDate>Fri, 23 Sep 2022 09:21:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/09/lecture-3-unconstrained-optimization/</guid>
      <description>3.1 无约束优化概述 线性回归
均方误差MSE，具有好的几何特性（试图找到一条直线，使平面上点到直线的欧式距离和最小） $$min_{w,b} \sum_{i=l}^n [f(x_i)-y_i]^2$$ 最小二乘是关于$w,b$的无约束优化问题 根据实际问题确定不同优化目标
误差绝对值和最小，MAE $$min_{w,b} \sum_{i=l}^n |w^Tx_i+b-y_i|$$ 保证最大偏差最小化，$$min\ max_{w,b} \sum_{i=l}^n |w^Tx_i+b-y_i|$$ 线性回归解不唯一时：带正则项的线性回归选择性质不同的解
想要得到一个稀疏的解，解中非零的个数尽可能少，可以用$l_0$和$l_1$范数，其中$\widehat{w}=&amp;lt;w,b&amp;gt;$ $$min_{w,b} \sum_{i=l}^n [f(x_i)-y_i]^2+\mu||\widehat{w}||_1$$ 平衡模型拟合和解的光滑性，使用$l_2$范数，使目标函数是严格凸的，利于优化 $$min_{w,b} \sum_{i=l}^n [f(x_i)-y_i]^2+\mu||\widehat{w}||_2^2$$ Logistic回归
$h(x)$表示样本为+的概率，$1-h(x)$表示样本为-的概率； 用线性回归预测的值去逼近真实类别的对数比值，也称之为对数几率回归； $$h(x)=\frac{1}{1+e^{-z}}=\frac{1}{1+e^{w^Tx+b}}$$
$$ln\frac{h(x)}{1-h(x)}=w^Tx+b$$
如何确定$h(x)$中$w,b$的值？
如果用MSE，其不是凸函数，所以逻辑回归用对数似然来求参数最优 $$max\ ln L(w,b)=\sum_{i}^{n}y_i lnh(x_i)+(1-y_i)ln(1-h(x_i))$$
$$min_{w,b}\ -ln L(w,b)=-\sum_{i}^{n}y_i lnh(x_i)+(1-y_i)ln(1-h(x_i))$$
可见逻辑回归的对数似然是一个无约束优化问题，且海森阵半正定，所以其也是一凸优化问题。
有约束优化问题可以转化成无约束来用无约束优化求解技术进行求解。有哪些转化呢？ 3.2 无约束优化的一阶最优条件 最优解
类别：全局最优、局部最优（$\delta$邻域） 通常全局最优是很难的，验证一个点是局部最优： 与附近所有点的函数值进行比较&amp;mdash;NO!； 光滑函数可利用函数性质看是否局部最优，比如一阶导数（梯度），二阶导数（海森矩阵）； 一阶必要条件：若$x^\ast$为局部极小点，则$\nabla f(x^\ast)=0$；
目标函数在局部极小点处的任意方向导数均为0，即目标函数的切平面是水平的。
什么叫方向导数？为0的不是梯度向量吗？ 对任意方向$d$, 有$d^T\nabla f(x^\ast)=\mathbf{0}$； 稳定（驻）点：对于多元函数，梯度为0的点也一样，未必是极值点（极大、极小），可能是鞍点，因此确定一个稳定点是极值点，还需要确定该点处的二阶导数；
3.3 无约束优化的二阶最优条件 二阶必要条件：若$x^*$为局部极小点，则$f$在$x^\ast$的邻域内二阶可微，则$\nabla f(x^\ast)=0$且$\nabla^2f(x^\ast)$半正定。
一二阶必要条件的证明：反证法，利用方向向量+泰勒展开证明；
泰勒的一元和多元展开
二阶充分条件：$f$二阶连续可微（二阶只说可微呢？），$\nabla f(x^\ast)=0$且$\nabla^2 f(x^\ast)$正定，则$x^\ast$是严格局部极小点；
必要条件是否是充分条件的一个典型反例：$f(x)=x^3$；
二阶充分条件不一定是二阶必要条件，所以对于无约束优化问题，不存在二阶的充分必要条件，例如$f(x)=x^4$；
极大值点，二阶负定；</description>
    </item>
    
    <item>
      <title>An Introduction to Optimizaiton</title>
      <link>https://optstats.github.io/2022/09/an-introduction-to-optimizaiton/</link>
      <pubDate>Fri, 23 Sep 2022 03:16:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/09/an-introduction-to-optimizaiton/</guid>
      <description>Chapter 6. Basics of Set-Constrained and Unconstrained Optimizaiton 6.1 Introduction objective function/cost function
decision variables
constraint set/feasiable set
extremizers
minimizer maximizer constrained optimization problem
unconstrained opitmization problem: $\Omega=R^n$
minimizer
local minimizer What we often to find multiple global minimizer It is difficult to find. strict 6.2 Condition for Local Minimizers gradient and Hessian
feasiable direction
Why: a minimizer may be lie either in the interior or on the boundary of $\Omega$.</description>
    </item>
    
    <item>
      <title>Lecture 5 Empirical distributions and dimensionality reduction</title>
      <link>https://optstats.github.io/2022/09/lecture-5-empirical-distributions-and-dimensionality-reduction/</link>
      <pubDate>Tue, 20 Sep 2022 03:11:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/09/lecture-5-empirical-distributions-and-dimensionality-reduction/</guid>
      <description> 测度：是一个实值函数，映射一个集合到一个实数值，且满足三条性质
非负：$u(A)\geq 0,\forall A\in X$ 可加性：$u(A\cup B)=u(A)+u(B),\forall A,B\in X$ $u(\emptyset)=0$ 概率测度：在整个空间上，测度的值为1；
Dirac $\delta$函数：是一个没有密度的概率测度，任何含有点$x$的集合其$\delta_x$的值为1，$\delta_x$也当做点质量；
经验测度：对于样本$X_1,X_2,&amp;hellip;X_n$，其为
$$\mathcal{P}_ n(A)=\frac{1}{n}\sum_{i=1}^n \delta_{X_i}(A)=\frac{1}{n}\sum_{i=1}^n I_A(X_i)$$
如何理解经验测度？
不知道客观真实的概率测度，因此收集一系列样本并计算相对频率； 指示函数或$\delta$函数表示一种计数功能； 经验测度是一个概率测度：$\mathcal{P}_ n(A)=\frac{1}{n}\sum_1^n\delta_{X_i}(A)=\frac{1}{n}\sum_{i=1}^n 1=\frac{1}{n}n=1$ 期望：对于一个分布$\mathcal{P}$，随机变量$X$可取值$\lbrace x_1,x_2,&amp;hellip;x_n\rbrace$，随机变量将样本空间的一个输出或几个输出映射到其一个值，$\mathcal{P}(x)$表示随机变量取值为$x$的概率，下面的$d\mathcal{P}(x)$表示$X=x$时的概率值，也是密度函数的积分单位区域面积值；
$$\mathbf{E}[X]=\int xd\mathcal{P}(x)=\int xp(x)dx$$
经验期望（样本期望）：一般是算样本的均值，作为总体期望的近似； </description>
    </item>
    
    <item>
      <title>Distribution Zoo</title>
      <link>https://optstats.github.io/2022/09/distribution-zoo/</link>
      <pubDate>Thu, 15 Sep 2022 11:56:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/09/distribution-zoo/</guid>
      <description>Bernoulli distribution
对于一次随机试验，只有两种可能的输出，一个参数$\theta$刻画成功的概率 Binomial distribution
进行$n$次随机实验，单次实验皆为伯努利试验，且伯努利成功概率参数为$\theta$，求$t$次成功的概率 $Bin(n,\theta)$ Categorical Distribution
对于一次随机试验，试验的输出可能有$k$种，$k\geq 2$ Multinomial distribution
进行$n$次随机试验，单次实验皆为类别分布试验，假设$k$类，因为$n$次实验，每类都有一定数目的输出$\lbrace x_1,x_2&amp;hellip;x_k\rbrace$，$p_i$表示在单次实验里第$i$类出现的概率 $p=\frac{n!}{x_1! x_2!&amp;hellip; x_k!}p_1^{x_1} p_2^{x_2}&amp;hellip;p_k^{x_k}$ 可以理解成基于一个多重集合排列问题的概率求解 Beta distribution
描述一个概率的概率分布：例如，不知道硬币是否均匀，做多轮抛掷，每轮很多次抛投都有一个正面朝上概率，从而在多轮上得到关于正面朝上概率的分布； 二项分布可以看成进行多重伯努利实验得到的分布，beta分布可以看成进行多重二项分布得到的分布； 二项分布中，概率是参数，beta分布中，概率是随机变量：这里体会参数和变量的区别，想象一次函数的$k$和$x$，参数一般是控制模型且确定的数； Dirichlet distribution</description>
    </item>
    
    <item>
      <title>Bayesian Weak Supervision via an Optimal Transport Approach</title>
      <link>https://optstats.github.io/2022/09/bayesian-weak-supervision-via-an-optimal-transport-approach/</link>
      <pubDate>Tue, 13 Sep 2022 11:56:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/09/bayesian-weak-supervision-via-an-optimal-transport-approach/</guid>
      <description>Introduction weak supervision
definition two-stage weak supervision
first to infer the latent truth label then the latent label are used to train model weak source
human experts automated heuristics If the sources are humans, we would like to obtain the ability of labelers.
To see the paper how to model the relation between the latent truth label and weak label as the OT problem.
post-hoc analysis
The analysis afterwards you have done the researh, as some extra conclusions and analysis.</description>
    </item>
    
    <item>
      <title>Bayesian Inference with Optimal Maps</title>
      <link>https://optstats.github.io/2022/09/bayesian-inference-with-optimal-maps/</link>
      <pubDate>Sat, 10 Sep 2022 09:56:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/09/bayesian-inference-with-optimal-maps/</guid>
      <description>Abstract How to do Markov chain simulation and how to avoid it? The map how to push forward the prior measure to the posterior measure. how about the existence and uniqueness of measure-preserving map? How to explicitly parameterize the map and effciently compute? The computational bottlenecks of MCMC. </description>
    </item>
    
    <item>
      <title>Lecture 2 Cyclical Monotonicity and Kantorovich Problem</title>
      <link>https://optstats.github.io/2022/09/lecture-2-cyclical-monotonicity-and-kantorovich-problem/</link>
      <pubDate>Wed, 07 Sep 2022 07:56:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/09/lecture-2-cyclical-monotonicity-and-kantorovich-problem/</guid>
      <description></description>
    </item>
    
    <item>
      <title>OT CLEANER: LABEL CORRECTION AS OPTIMAL TRANSPORT</title>
      <link>https://optstats.github.io/2022/09/ot-cleaner-label-correction-as-optimal-transport/</link>
      <pubDate>Mon, 05 Sep 2022 04:56:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/09/ot-cleaner-label-correction-as-optimal-transport/</guid>
      <description>Abstract Noisy label affects the model generalization; Label correction as a method to handel it but fail to handel, heavy noise samples with many categories What is global label distribution? The work formulates the label correction to an optimal transport problem; Having the superiority on training efficiency and classification accuracy; </description>
    </item>
    
    <item>
      <title>Lecture 1 Introduction to Optimal Transport</title>
      <link>https://optstats.github.io/2022/09/lecture-1-introduction-to-optimal-transport/</link>
      <pubDate>Fri, 02 Sep 2022 04:56:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/09/lecture-1-introduction-to-optimal-transport/</guid>
      <description>Monge Problem Background To transport a pile of sand into some holes, such as from point $x$ to $T(x)$; We concern which in an efficient way to transport formulated with cost $x-T(x)$; The over all transport formulated by $\int [x-T(x)]f(x)dx$; We could understand the formula above in this way, the $x-T(x)$ denotes the unit mass transport cost, the $f(x)$ denotes the mass on opint x, so the whole transport cost on point $x$ is $[x-T(x)]f(x)$.</description>
    </item>
    
    <item>
      <title>Lecture 16 Gradient Flow</title>
      <link>https://optstats.github.io/2022/09/lecture-16-gradient-flow/</link>
      <pubDate>Thu, 01 Sep 2022 04:01:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/09/lecture-16-gradient-flow/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Lecture 15 Wasserstein Curves</title>
      <link>https://optstats.github.io/2022/09/lecture-15-wasserstein-curves/</link>
      <pubDate>Thu, 01 Sep 2022 02:01:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/09/lecture-15-wasserstein-curves/</guid>
      <description> 对于W-空间两个分布$u_0$和$u_1$的两个csg，$u_s$和$u_t$： $$W_p(u_s,u_t)\leq |t-s|W_p(u_0,u_1)$$
Curves in $W_p$ 考虑$u_t,t\in (0,1)$是$W_p(X)$空间的一个连续曲线，其中$p&amp;gt;1$，$X$是凸的支撑集，则存在一个向量场$v_t$，曲线沿着向量场流动生成 $$\partial_t u_t+\nabla(v_tu_t)=0$$
关于csg的思考 为什么说是考虑成对测度：因为csg是基于最优耦合去构造的，需要一对分布去得到csg； 如果将csg看做一个连续曲线，则可以将其动态生成（有t）与向量场关联，每个$u_t$的梯度是下一个方向，也是最优下降方向即$u_t^{&amp;rsquo;}=-\delta \nabla u_{t-1}$； 曲线生成过程是一个用离散采样逼近连续的过程，将最优耦合看作一个状态转移映射$f$， $$(u_t,u_{t-1})\sim \pi^*_{t,t-1}$$
$$u_t\sim f(u_{t-1})$$
$$min_f E_{\pi^*} ||u_t-f(u_{t-1})||^2$$
梯度和梯度流基础
离散的时间$k\in Z_0\lbrace0,1,2,,,\rbrace$，连续时间$t\in R_0=[0,\infty)$，离散时间可以看作是连续时间的一个离散化，基于一个无穷小的步，$t=\delta k,\ \delta&amp;gt;0$； 一个空间上的动力学是一个随着时间变化的状态曲线； 状态曲线是如何随着时间生成或者变化的？ 状态曲线是连续时间场景的梯度流生成，但我们用离散时间的梯度下降算法去近似连续时间的梯度流微分方程的光滑解； 状态转移方向：向量场方向，$dX_{t}=F(X_t)=-\nabla X_t$，其中$F$是向量场，表示下一刻向哪里运动，也即向最速下降方向； 连续两个离散时间点的转移关系：注意连续时间没有下一刻的概念，这里用一种线性近似去拟合,因此有$X_{t+\delta}=X_t+\delta F(X_t)$； 如果从$t=0$开始，则生成一个$X_t(X_{\delta t})$序列且满足$X_{t+1}=X_t+\delta F(X_t)$，注意梯度下降的步长和时间变化步长作用类似，所以用相同 $\delta$ 表示； 梯度流到底在做什么数学问题：连续时间的凸优化问题的求解，按照最速下降的方向去优化目标； 梯度流和梯度下降可以看出：在离散和连续两个赛道的优化，可以并发处理且关联上，且保持相同的分析结果，如收敛分析，但离散情形要做一些假定，如函数是$L$-平滑等； 负梯度方向对应的优化问题形式 $$-\nabla f(x)=argmin_{v\in R^d}\lbrace&amp;lt;\nabla f(x),v&amp;gt;+\frac{1}{2}||v||^2\rbrace$$
Reference Gradient flow and gradient descent </description>
    </item>
    
    <item>
      <title>Lecture 13 Scaling Algorithm for Entropic OT</title>
      <link>https://optstats.github.io/2022/08/lecture-13-scaling-algorithm-for-entropic-ot/</link>
      <pubDate>Wed, 31 Aug 2022 12:29:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/08/lecture-13-scaling-algorithm-for-entropic-ot/</guid>
      <description>Sinkhorn&amp;rsquo;s Algorithm for Entropic Optimal Transport 对于熵正则最优传输 $$min_{\pi}\sum_{ij} \pi_{ij}c_{ij}+\epsilon\sum_{ij}\pi_{ij}log\pi_{ij}$$ $$s.t.\ \mathbf{\pi}\mathcal{1}=a$$ $$\mathbf{\pi}^T\mathcal{1}=b$$
其最优解唯一，形式为$\pi^\ast_{ij}=u_iK_{ij}v_j$，其中$K_{ij}=exp(c_{ij})\over\epsilon$，且最优传输矩阵
$$\pi^\ast=Diag(u)KDiag(v)$$
熵正则最优解形式推导 写出原问题的拉格朗日函数 $$L(\pi,u,v))\sum_{ij} \pi_{ij}c_{ij}+\epsilon\sum_{ij} \pi_{ij}log \pi_{ij}-u(\pi\mathbf{1}-a)-v(\pi^T\mathbf{1}-b)-\epsilon(\sum_{ij} \pi_{ij}-1)$$
处于方便计算，上式体现传输矩阵有个正则化约束项，可改写熵正则的目标问题等价$\epsilon\sum\pi log\pi-1$，基于此，可以理解为什么拉格朗日惩罚系数是$\epsilon$；
对Larg函数求最优点，注意$\pi_{ij}$求导，直接看线性结构的系数（向量分量）；
$$\frac{\partial L}{\partial\pi_{ij}}=c_{ij}+\epsilon(log\pi_{ij}-1)-u_i-v_j-\epsilon=c_{ij}+\epsilon log\pi_{ij}-u_i-v_j=0$$
$$log\pi^\ast_{ij}=\frac{-c_{ij}+u_i+v_j}{\epsilon}$$
$$\pi^\ast_{ij}=exp(\frac{-c_{ij}+u_i+v_j}{\epsilon})=exp(\frac{u_i}{\epsilon})exp(-\frac{c_{ij}}{\epsilon})exp(\frac{v_j}{\epsilon})$$
Scaling Algorithm
我们要想求解最优传输矩阵$\pi$，通过上面正则解的推导，得到最优矩阵的元素$K_{ij}$与$u_i,v_j,c_{ij},\epsilon$有关；因此，如果恢复矩阵$\pi$，我们关键需要求解向量$u$和$v$，即产生以下约束问题： $$u= [exp(\frac{u_i}{\epsilon})]$$ $$v= [exp(\frac{v_j}{\epsilon})]$$ $$s.t.\ Diag(u)KDiag(v)\mathcal{1}=a $$ $$Diag(v)K^TDiag(u)\mathcal{1}=b $$
理解上述式子需要注意：
$K$矩阵元素与代价矩阵元素有关； 对角阵左乘一个矩阵，等价于矩阵的每行乘对角阵对应元素； 对角阵右乘一个矩阵，等价于矩阵的每列乘对角阵对应元素； $Diag(u)KDiag(v)=\pi$，对于传输矩阵$\pi$的元素$\pi_{ij}=u(i)K_{ij}v(j)$，即$\pi_{ij}$由三部分构成，可以考虑计算结合过程为$K_{ij}$的$i$对应着$u(i)$，$K_{ij}$的$j$对应着$v(j)$； 第四个式子可以看成第三个式子的转置，结合传输矩阵转置与单位列向量乘即对逐列求和可得； 结合以上，我们知道等价于解关于$u$和$v$的方程组： $$Diag(u)Kv=a$$ $$Diag(v)K^Tu=b$$
对此，交替迭代的Sinkhorn算法如下：
初始化$v^0=(1,1,1)^T$； 基于第一个方程更新$u^1$； 基于第二个方程更新$v^1$； 在$l+1$轮，用$v^l$更新$u^l$，根据$u^{l+1}$来更新$v^{l+1}$，得到$(v^{l+1},u^{l+1})$，从而得到相应的$\pi$阵；
算法的几何解释：
在两个空间交替投影（找到适合问题的解），一个空间是行和为$a$的矩阵空间，一个空间为行和为$b$的矩阵，算法最终收敛在两个空间相交的地方，即表示行和为$a$且列和为$b$的传输矩阵； 算法理论基础：
两个闭凸集$C$和$D$，假设在这两个闭凸集上的投影算子$P_C$和$P_D$，当进行迭代交替的投影时，最终两个投影输出会落在$C\cap D$上； 更多解释可参考Note； </description>
    </item>
    
    <item>
      <title>Lecture 12 Entropy and Second Law</title>
      <link>https://optstats.github.io/2022/08/lecture-12-entropy-and-second-law/</link>
      <pubDate>Wed, 31 Aug 2022 10:20:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/08/lecture-12-entropy-and-second-law/</guid>
      <description>Entropy Regularized Optimal Transport 香农熵：一个分布的香农熵在有限空间$\sum_{i=1}^k p_i=1$为 $$-\sum_i p_ilogp_i$$
分布越均匀，熵越大，均匀分布的熵最大，分布越偏，熵越小； 对于两个分布的耦合$\pi$，其最优传输问题为： $$min_{\pi}\sum_{ij} \pi_{ij}c_{ij}$$ $$s.t.\ \mathbf{\pi}\mathcal{1}=r$$ $$\mathbf{\pi}^T\mathcal{1}=c$$
对应的熵正则最优传输为： $$min_{\pi}\sum_{ij} \pi_{ij}c_{ij}+\epsilon\sum_{ij} \pi_{ij}log \pi_{ij}$$ $$s.t.\ \mathbf{\pi}\mathcal{1}=r$$ $$\mathbf{\pi}^T\mathcal{1}=c$$
原始的离散最优传输问题是一个线性规划问题，其最优解在约束凸集的顶点处取得； 分析带有熵正则的最优传输，对于一个耦合，为什么顶点处的熵是零； 如何分析熵正则的最优传输将最优点从边界顶点处推向约束凸集的内部； </description>
    </item>
    
    <item>
      <title>Lecture 14 Wasserstein Space</title>
      <link>https://optstats.github.io/2022/08/lecture-14-wasserstein-space/</link>
      <pubDate>Tue, 30 Aug 2022 02:01:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/08/lecture-14-wasserstein-space/</guid>
      <description> 度量：是一个函数，定义为一个距离，满足以下，
对称$d(x,y)=d(y,x)$； 非负仅当$d(x,x)=0$； 三角不等式 Wasserstein 空间$(X,D)$：参考度量空间的概念，可以理解为在样本空间$X$上，定义度量D为Wasserstein距离；
n维空间上的度量：$L$-$p$范数，如2-范数，无穷范数等，有的范数基于内积定义，内积的好处在于可以定义距离和角度等一系列测度；
分布的距离 Total variation distance：相同支撑维度，对应支撑测度差的绝对值和；
Hellinger distance：2-范数的一半；
上面两个距离无法区分下图分布之间距离，分析为什么； 最优传输距离：通过最优传输来定义两个分布$a,b$的距离为$W_p(a,b)$，称为$p$-wasserstein距离，$p\geq1$，一般取$p=2$；
最优传输研究两个测度之间的距离，这种问题的定义形式构成一个W-dis； W-dis是一个度量 构造耦合的技巧值得参考，结合下图 考虑三个分布$a,b,c$即$\sum a_i=\sum b_i=\sum c_i=1$，定义一个耦合$S=\pi^\ast diag{\frac{1}{\tilde{b}}}\gamma^\ast$，也就是要检查这个耦合的两个边缘测度是分布$a$和分布$c$； 曲线 找两个分布的中介曲线，如下图，如果直接是分布的凸组合，得到的更像一个混合模型； 借用优化的观点来思考这个问题，如求二维空间两个点$x,y\in R^2$的中点$z=\frac{x+y}{2}$，优化模型为 $$z=argmin_z ||z-x||^2+||z-y||^2$$ 通过梯度$dz$可以得到$z=\frac{x+y}{2}$；
Barycenter：距离多个分布平均W-距离最小的分布； $$b^{\ast}=argmin_{b}\sum_{i=1}^n \theta_i W_p^p(b,a_i),\ \sum\theta_i=1$$ 可见所求的barycenter是在多个分布构成的凸集内；
constant speed geodesic(csg)
定义：给定两个分布$a_0$和$a_1$，其csg为一个分布族$\lbrace a_t\rbrace_{t\in(0,1)}$，且满足$$W_p(a_0,a_t)=tW_p(a_0,a_1)$$ 如何构造得到$a_t$：从$a_0$和$a_1$的最优耦合$\pi^\ast$推导； 对于有限样本支撑空间$\mathcal{X}=\lbrace x_1,x_2,,,x_n\rbrace$， $$P_t:\ \mathcal{X}*\mathcal{X}\rightarrow \mathcal{X}$$ $$(x,y)\rightarrow tx+(1-t)y$$ 定义$a_t=\mathbf{E}_{\pi^\ast} P_t(X,Y),\ X\sim a_0,\ Y\sim a_1$ $$a_t=\sum_{ij}^n\pi_{ij} P_t(X,Y), \ (X,Y)\sim \pi^{\ast}$$
$$W_p(a_0,a_t)=tW_p(a_0,a_1)$$
$$W_p(a_{t1},a_{t2})=(t_2-t_1)W_p(a_0,a_1)$$
对csg的构造分析 csg分布是基于两个边缘分布及其最优耦合构造得到； csg是指使$W_p$满足线性关系的分布族； $P_t$是产生两个分布的凸组合分布，这个分布位于两个端点分布之间； </description>
    </item>
    
    <item>
      <title>Lecture 13 罚函数法</title>
      <link>https://optstats.github.io/2022/08/lecture-13-%E7%BD%9A%E5%87%BD%E6%95%B0%E6%B3%95/</link>
      <pubDate>Sun, 28 Aug 2022 23:01:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/08/lecture-13-%E7%BD%9A%E5%87%BD%E6%95%B0%E6%B3%95/</guid>
      <description>导论 罚函数法更加通用，对目标函数和约束没有限制，相比如二次规划要求目标是二次凸，可行方程线性； 基本思想：用无约束优化方法来解决有约束优化问题； 惩罚项：对可行集的刻画，非可行集内的点，有偏离，把这种偏离记录下来，用二次记录偏离值； 罚函数法基本形式 1. 只有等式约束的优化问题 $$min\ f(x)_{x\in S}$$
$$s.t.\ h_i(x)=0, \ i=1..l$$
构造关于约束等式的惩罚项$p(x)=\sum_{i=1}^lh_i(x)^2$，如果$x\in S$则$p(x)=0$，如果$x\notin S$则$p(x)\neq 0$；因此得到罚函数
$$P(x,\sigma)=f(x)+\sigma p(x)=f(x)+\sigma \sum_{i=1}^lh_i(x)^2$$
可以理解$p(x)$刻画可行性破坏程度，$\sigma$表示惩罚力度；
因此，产生新的无约束优化问题 $$min\ P(x,\sigma)_{x\in R^n}=f(x)+\sigma p(x)=f(x)+\sigma \sum_{i=1}^l h_i(x)^2$$
需要注意的是，新的无约束优化问题对$x$没有限制，可行域是$R^n$，而原问题是$x\in S$； 惩罚系数一般从小往大取，如果初始值很大，在非可行域内，罚函数最优值偏离原问题的最优值会很大； 最值分析 $min_{x\in S} f(x)=min_{x\in S}f(x)+\sigma p(x)\geq min_{x\in R^n}f(x)+\sigma p(x)$
因为无约束优化的可行范围更大，所以无约束优化问题是原问题的一个下界； 无约束惩罚函数的最值是关于$\sigma$的函数，其是原问题的下界，我们想得到一个紧致的下界，所以要关于$\sigma$求max； 罚函数关于$\sigma$的分析 关于$\sigma$是一个单调增函数：$Q(\sigma)=min_x f(x)+\sigma p(x)$，$Q$是原问题最优值的下界； 我们想求关于$\sigma$的最大等价于有：$max_{\sigma} Q(\sigma)\iff lim_{\sigma\rightarrow\infty} Q(\sigma)$ 一般在关于$x$求最优点（一阶梯度和二阶海森矩阵来判断）后得到最优点是关于$\sigma$的表达式，令$\sigma\rightarrow\infty$得到最优点？ 实例结论：目标梯度和约束函数梯度线性相关，则符合KKT条件； 罚函数法的算法 s1：初始值$x_0,\sigma_0,\epsilon,k=1$； s2：迭代求解，先求$min\ P(x,\sigma)$一个可行解$x_k$； s3：如果$\sigma_kp(x_k)\leq\epsilon$则终止，即该解在惩罚项的可接受阈值达到； s4：如果s3未终止，则进行更新转s2继续迭代：更新$\sigma_{k+1}&amp;gt;\sigma_k$，$k=k+1$； 更新$\sigma_{k+1}$的策略 $\sigma_{k+1}=\beta\sigma_{k}$，$\beta$为常数； 考虑为什么：增大幅度取决于$min\ P(x,\sigma)$的求解难度，比如 易解$\sigma_{k+1}=10\sigma_{k}$ 难解$\sigma_{k+1}=2\sigma_{k}$ 罚函数法数学分析（详见拓展阅读） 关于最优解$x_k$序列的函数序列单调性分析
${P(x_k,\sigma_k)}$：上升 ${p(x_k)}$：下降 ${f(x_k)}$：上升 聚点收敛性质：$x_k$是问题$P(x,\sigma)$的全局最优解，则${x_k}$的任意聚点$\bar x$是$min_{x\in S}\ f(x)$的全局最优解；</description>
    </item>
    
    <item>
      <title>Lecture 10 对偶理论</title>
      <link>https://optstats.github.io/2022/08/lecture-10-%E5%AF%B9%E5%81%B6%E7%90%86%E8%AE%BA/</link>
      <pubDate>Sun, 28 Aug 2022 15:01:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/08/lecture-10-%E5%AF%B9%E5%81%B6%E7%90%86%E8%AE%BA/</guid>
      <description>对偶问题基础 为什么要构造原问题的对偶问题？
非凸问题NP-hard：对偶问题便于计算分析； 对偶问题解与原问题解的关联； 对偶将一系列约束优化转化成更少的约束优化甚至无约束优化； 对偶提供一些新的解法； 如何构造对偶
拉格朗日对偶 Fenchel对偶 对偶问题与原问题的关系
对偶
对偶不仅适用于连续优化，整数规划等也适用对偶理论； 鲁棒优化、锥优化很需要对偶分析 拉格朗日对偶 拉格朗日函数 $$L(x,\lambda,\mu)_{x\in X}=f(x)+\sum_i^m \lambda_i g_i(x)+\sum_i^l \mu_i h_i(x),\ \lambda_i\geq 0$$
将原问题的约束条件转化通过乘子合并为一个新的目标函数，减少约束，只剩$x\in X$； 对拉格朗日函数关于$x\in X$求最小值，要比原问题带有多个约束要简单，并且给一组$(\lambda,\mu)$的值，就可以得到关于$x$的最小值，最小值是关于$(\lambda,\mu)$的函数； 对偶函数 $d(\lambda,\mu)=min\ L(x,\lambda,\mu)_{x\in X},\ \forall\lambda,\mu,\lambda\geq0$
$d(\lambda,\mu)=min\ L(x,\lambda,\mu)_{x\in X}\leq min\ L(x,\lambda,\mu)_{x\in S}\leq min\ f(x)_{x\in S}$ 其中$S$为原问题的可行域（不仅是$x\in X$还有约束条件）
上式第一个不等号因为在一个可行域大的集合（$S\subseteq X$）里肯定能取到更小的值； 上式第二个不等式因为在可行域$S$里，两个乘子项部分是$\leq0$的； 结论：$\forall\lambda,\mu,\lambda\geq 0$，$d(\lambda,\mu)\leq V(p)$，$V(p)$是原问题最优值，对偶问题是原问题最优值的下界，通过不同的$\lambda,\mu$可以算出不同的下界；
拉格朗日对偶问题：在$d(\lambda,\mu)$许多个下界中找出最大下界
$$max\ d(\lambda,\mu)=max_{\lambda\geq 0,\ \mu\in R^n}min_{x\in X}L(x,\lambda,\mu)$$
如果先对$\lambda,\mu$先求最大，再对$x$求最小呢？：还原成原问题 原问题可以看成拉格朗日函数先对乘子求最大，然后对变量求最小，即$min\ max$的问题 对偶问题可以看成拉格朗日函数先对变量求最小，然后对乘子求最大，即$max\ min$的问题 </description>
    </item>
    
    <item>
      <title>Lecture 3 凸函数</title>
      <link>https://optstats.github.io/2022/08/lecture-3-%E5%87%B8%E5%87%BD%E6%95%B0/</link>
      <pubDate>Sat, 27 Aug 2022 14:01:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/08/lecture-3-%E5%87%B8%E5%87%BD%E6%95%B0/</guid>
      <description>凸函数定义 在凸集上定义的关系；
几何意义：输入两点凸组合的值在两点函数值的凸组合下方 凹凸关系：一般极大化凹函数问题转化成极小化凸问题； 严格凸：取等，想象输入两点线段上的值，输出等于两点的凸组合，如线性函数，而不是想象函数值水平相等（极特殊情形）；
常见凸函数 线性函数：既是凹函数也是凸函数，常作为非凸的近似 二次函数：$x^TQx+ax+b$，$Q$半正定 最小二乘$||Ax-b||^2_2$：有$A^TA$半正定 范数：$p\geq 1$，一般在优化问题中，考虑1-范数和2范数较多； 凸函数的性质 凸函数表明其是连续函数； 凸函数$f\iff\phi(\alpha)=f(x+\alpha y)$是凸函数，其中$\phi$是一元函数； 经过$x$沿$y$方向上$\alpha$倍的值 降维的思想，参考boyd讲义； 构成凸函数的一个充要条件：$f(y)\geq f(x)+\nabla f(x)^T(y-x)$ 图像在切平面之上 证明 已知凸函数推不等式：泰勒展开，取$\lambda \rightarrow 0$的极限 满足不等式推凸函数：凸组合 构成凸函数的二阶充要条件 非空开集 二阶可微 任意点处海森矩阵半正定 关于梯度的不等关系关系：泰勒展开 保凸运算 凸函数构成的透视函数 凸函数的非负组合 一组凸函数求最大 凸集与凸函数关联 凸函数的下水平集$\lbrace x|f(x)\leq a\rbrace$是凸集
注意是对x，如果是二维，是下水平集投影在x轴上区间； 如果是非凸函数，下水平集投影在x轴上的区间是断开的（因为$f(x)\geq a$）； 反之不成立：下水平集是凸集的不一定是凸函数，想象鹰图； 上图：包含边界$y=f(x)$的上部区域$y\geq f(x)$构成的集合
几何上看，从边界出发一系列的射线； 对非空闭凸集，投影定理描述的距离函数$f(y)=min\lbrace||y-x||_2,x\in C\rbrace$是凸函数；
m个凸函数的前k大凸函数的和构成的函数是凸函数；</description>
    </item>
    
    <item>
      <title>Lecture 2 凸集</title>
      <link>https://optstats.github.io/2022/08/lecture-2-%E5%87%B8%E9%9B%86/</link>
      <pubDate>Sat, 27 Aug 2022 02:01:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/08/lecture-2-%E5%87%B8%E9%9B%86/</guid>
      <description>引入：凸优化问题 可行域凸集； 局部最优即是全局最优; 最优点处的负梯度方向与凸可行集上任意方向的夹角大于90°，非凸集未必成立； $$-\nabla f(x^\ast)^T(x-x^\ast)\leq0, \forall x\in S$$ 上式也是最优点成立的等价条件； 基本定义 凸集：集合中两点的连线仍在集合内，对$\forall x,y\in C,\ \forall \lambda\in[0,1]$有：$\lambda x+(1-\lambda)y\in C$；
凸组合：系数非负，且和为1；
凸包：对任意集合里的点，凸组合构成的集合，是一个凸集，可以理解成将非凸集给填充成一个凸集；
对非凸的结构凸化：对整数可行集凸包化成一个多边形，利用线性规划解决； 如何构造凸包？ 常见凸集
超平面$\lbrace x|a^Tx=b,a\neq 0\rbrace$ 半空间$\lbrace x|a^Tx\geq b,a\neq 0\rbrace$ 多面体：多个线性不等式构成的集合区域； 欧几里得球：同boyd讲义，$||u||_2$表示球心为0，半径为1的单位球，球心平移的操作得到$\lbrace x_c+ru|\ ||u||_2\leq1\rbrace$； 椭球：正定矩阵的特征值开方即是椭球的半轴长； 二阶锥： 锥：$x\in C, \lambda\geq 0, \lambda x\in C$ 二阶锥定义：$\lbrace(x,t)|\ ||x||_2\leq t\rbrace$，思考二维$x$构成的几何结构$\sqrt{x_1^2+x_2^2\leq t}$； 半正定锥$S_{+}^n$：$X\geq 0\iff z^TXz\geq 0, \forall z$ 练习：线性规划的最优解组成的集合是凸集； 保凸运算
两个凸集的交 两个凸集元素的加减构成的集合 一个线性的绝对值不等式可以看成两个半空间的交 放射变换：有偏差的线性变换$f(x)=Ax+b$ 凸集上的放射变换得到集合是凸集 凸集上的逆放射变换得到的集合（原像）是凸集 放缩：想象二维的放缩，如锥束一样； 平移：凸集的移动，不改变凸性； 投影：有种降维操作的感觉，想象三维空间的球凸集，投影到二维平面的圆； 凸集基本性质 投影定理：在非空闭凸集上存在唯一的一点到该集合外的任一点距离最小；
存在且唯一
连续函数在有界区域上求最值，存在 投影点的充要条件：$(x-x^\ast)^T(y-x^\ast)\leq 0, \forall x\in C$</description>
    </item>
    
    <item>
      <title>Fundamentals of Topology</title>
      <link>https://optstats.github.io/2022/08/fundamentals-of-topology/</link>
      <pubDate>Sun, 21 Aug 2022 10:01:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/08/fundamentals-of-topology/</guid>
      <description>拓扑基础 拓扑空间：一个集合$X$构成一个拓扑空间，当它的子集族是开的且符合以下原子性质,
开集的无限并是开集 开集的有限交是开集 $X$和$\phi$是空集 对于一个开区间，两个端点，我们可以总是取距端点任意小距离的点，但总是不能取端点，开区间是开集；
闭集
开集的补是闭集，闭集的补是开集； 所有$X$的子集非开即闭？NO!&amp;hellip;$[0,2\pi)$非开非闭，考虑$(-\infty,0)\cup[2\pi,+\infty)$，前者是开的，后者是闭的； 对任意拓扑空间，$X$和$\phi$既是开也是闭的； $n-ball(x)$是开的，$n-ball$的定义是开界（强不等关系）的，所以开区间是开集的一种直观理解；
假定$X$是一个拓扑空间，$U$是$X$的子集，一些特殊的点：
$x\in U$，如果存在一个$n-ball(x)$，或者说，如果存在一个开集$\mathbf{O_1}$，$x\in \mathbf{O_1}$，$\mathbf{O_1}\subseteq U$，那么$x$称为$U$的内点；（可以体会$n-ball(x)$是开集的一种表示，如4.） 如果存在一个开集$\mathbf{O_2}$(也可说成关于$x$的开球)且$x\in\mathbf{O_2}$，如果$\mathbf{O_2}\subseteq X-U$，那么$x$为$U$的外点； 如果$x$既不是外点也不是内点，那么$x$是边界点，$U$所有边界点构成$U$的边界； 聚点：如果$x$是内点或是边界点，那么$x$是聚点； 所有聚点的集合称为集合$U$的闭包，是一个闭集，所以闭包是内点集合并上边界点集合； 例如，$X=R$时，边界点就是区间端点，注意集合$U$未必包含边界点，如$(0,1)$和$[0,1)$的边界点都是0和1；$(0,1)$内的所有点是内点；$[0,1]$内所有的点是聚点； 聚点名字limit-point由来：因为端点是一个序列的limit（级数）取到； 关于$R^n$的拓扑
对于任意的$n$，都可以构成一个拓扑空间； 基于开球的定义来刻画拓扑空间； 开球是开集的一种标准表示； 任意一个开集可以由开球的有限并得到，对于一维情况，一个开集可以由可数个开区间并得到； 子空间拓扑：通过开集和子集来构造； 平凡拓扑：$X$和$\phi$ 任意集合的拓扑（拓扑空间定义是很General的）：定义好基础的开集，就可以构造一系列开集，从而构成一个拓扑空间； Hausdorff原子性质：任意两个开集不相交，满足这个原子性质的拓扑空间构成Hausdorff空间； 拓扑空间上的的连续函数，令$f: X\rightarrow Y$是两个拓扑空间上的映射，对于任一集合$B\subseteq Y$,定义$B$的原像为 $$f^{-1}(B)=\lbrace x\in X|f(x)\in B\rbrace$$
对于分析连续函数繁冗的连续定义，拓扑的定义相对简洁； 函数通过用原像来表示，不要求函数是可逆的； 函数是连续的，当原像的映射$f^{-1}(B)$是一个开集； 同胚
两个拓扑空间如果存在一个双射，那么这两个空间是同胚的，同胚即一种等价关系（三种性质）； 单射：一对一映射； 满射：所有的y都是x对应； 双射：单射且满射（严格）； 非奇异线性变换？ 区间同胚：关于R的开区间是同胚的； 图的同构和拓扑同胚不总是一致成立； 流形
一个拓扑空间是一个流形，仅当：（1）空间中的每个元素都属于一个开集；（2）这个开集同胚与一个n维空间；（3）n称为流形的维度； 拓扑空间的构建：笛卡尔积，开集与开集的结合构成新的拓扑空间，因此流形的构建也可以来自笛卡尔积； 流形同胚时的几何图示？ Ref: UIUC planning algorithm </description>
    </item>
    
    <item>
      <title>线性规划-2</title>
      <link>https://optstats.github.io/2022/08/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92-2/</link>
      <pubDate>Sat, 20 Aug 2022 07:01:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/08/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92-2/</guid>
      <description>线性规划的解 可行解：满足约束条件和变量非负要求的解 ；
可行域（集）：全体可行解构成的区域（集合）；
最优解：使目标函数取得最值的可行解；
基：线性约束方程组的系数矩阵$A\in R^{m*n},m\leq n$的秩为m，则$A$的满秩子阵$B\in R^{m*m}$为一个基；
基变量 非基变量 基向量 基解：通过一确定的基，令非基变量为0，由约束方程组解出来的基变量，称为基解； 基可行解：满足变量非负条件的基解，基解不一定是基可行解； 可行基：对应于基可行解的基； 图解法 适用二维的线性规划问题，但几何直观
步骤：
画出可行域 线性规划若存在可行域，则是个凸集 画出目标函数 确定最优解：如果最优解存在，则一定在可行域的顶点取到 唯一最优解 无穷多最优解：目标函数平行于可行域的边界 无可行解：可行域和目标最值方向不一致 无界解：可行域是无界的 单纯性法 凸集的顶点：不能用凸组合表示的点 </description>
    </item>
    
    <item>
      <title>线性规划-1</title>
      <link>https://optstats.github.io/2022/08/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92-1/</link>
      <pubDate>Sat, 20 Aug 2022 05:01:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/08/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92-1/</guid>
      <description>规划问题的数学模型三要素 决策变量 目标函数 约束条件 线性规划 1. 定义 目标函数式决策变量的线性函数 约束条件是决策变量的线性不等式或线性等式 2. 标准型 $max\ c^Tx$
$s.t.\ Ax=b$
$x\geq 0$
极大化目标 全部约束为等式约束，且右端为非负 决策变量非负 3. 转换成标准型的操作 不等约束的转换： 加松弛变量 减剩余变量 松弛变量和剩余变量在目标函数中的系数为0 乘负号操作 min2max 变量为负 b为负 无约束变量的双替换 </description>
    </item>
    
    <item>
      <title>Lecture 2 Convex Set</title>
      <link>https://optstats.github.io/2022/08/lecture-2-convex-set/</link>
      <pubDate>Thu, 18 Aug 2022 11:01:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/08/lecture-2-convex-set/</guid>
      <description>凸集 仿射集：集合内任意两不同点构成的直线集 直线：$x=\theta x_1+(1-\theta)x_2,\ \theta\in R$
从方向向量角度理解直线的形成：如上式，$x_2$作为一点，$x_1-x_2$构成一个方向向量
$OC=\alpha OB+(1-\lambda)OA$的推导：点$\mathcal{C}$在线段AB内，点$\mathcal{O}$为直线AB外一点
$\mathcal{AC=OC-OA}$ $\mathcal{AB=OB-OA}$ $\mathcal{AC=\lambda AB}\Rightarrow \mathcal{OC-OA}=\lambda(\mathcal{OB-OA})$ $\mathcal{OC}=\lambda \mathcal{OB}+(1-\lambda)\mathcal{OA}$ $\mathcal{OC}$等向量可以看成C的向量坐标 二维平面上的直线是一维的，但是直线方程是二维的（从$x_i$的下标看），所以n维空间的直线是$n-1$维的，但方程是到$x_n$的；
仿射集可以表示成一个线性方程组的解（从定义出发思考）
凸集 线段：凸集的定义就是两点之间的点仍在该集合内，$x=\theta x_1+(1-\theta)x_2,\ \theta \in [0,1]$； 凸组合：全部系数和为1且非负； 凸包：凸集内点构成的全部凸组合集合； 锥：$x=\theta x_0, \theta\geq 0$； 锥组合：每个点系数非负构成的组合； 凸锥：系数和为1； 超平面：仿射且凸，$\lbrace x|a^Tx=b\rbrace(a\neq0)$，向量$a$定义平面的法向（也可以看做平面的方向） 半空间：$\lbrace x|a^Tx\leq b\rbrace(a\neq0)$ 欧几里得球：$B(x_c,r)=\lbrace x|\ ||x-x_c||_2\leq r\rbrace$ 椭球：$\lbrace x|(x-x_0)^TP^{-1}(x-x_0)\leq 1\rbrace$，$P\in S_{++}^n$，即对称正定 范数球：可以看成欧球的推广，$NB(x_c,r)=\lbrace x|\ ||x-x_c||\leq r\rbrace$ 范数锥：$\lbrace(x,t)|\ ||x||\leq t\rbrace$，欧几里得范数锥也称二阶锥 多面体：有限个线性不等式（半空间）和等式（超平面）构成的集合 半正定锥$\mathbf{S}^n_+$：关于矩阵对象的凸性研究 $\mathbf{S}^n$表示一个对称的$n*n$矩阵 半正定的$n*n$矩阵为一个凸锥：$\mathbf{S}^n_+=\lbrace X\in \mathbf{S}^n|X\geq 0\rbrace$ 正定：$\mathbf{S}^n_{++}=\lbrace X\in \mathbf{S}^n|X&amp;gt;0\rbrace$ 保凸运算
如何构建一个凸集：从常见凸集结合保凸运算构造； 任意凸集的交 基于凸集的前提下，其仿射函数的像或原像都是凸的； 线性矩阵不等式 放缩变换、投影 双曲锥 透视函数：$P: R^{n+1}\rightarrow R^n$ $P(x,t)=x/t \ \ dom(P)=\lbrace(x,t)|t&amp;gt;0\rbrace$ 线性分数函数：$f:R^n\rightarrow R^m$ $f(x)=\frac{Ax+b}{c^Tx+d}\ \ dom(f)=\lbrace x|c^Tx+d&amp;gt;0\rbrace$ 凸集的透视函数和线性分数函数的像或原像都是凸的； 适当锥(广义不等性) 非负多项式 每个元素非负的向量集合$\mathbf{R}^n_+$ 最小值和极小值 分离超平面定理：两个非空的不相交凸集，存在超平面可以分开两个凸集；注意更严格划分的要求</description>
    </item>
    
    <item>
      <title>Lecture 1 Introduction</title>
      <link>https://optstats.github.io/2022/08/lecture-1-introduction/</link>
      <pubDate>Thu, 18 Aug 2022 08:01:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/08/lecture-1-introduction/</guid>
      <description>1. 数学优化问题的定义和组成 2. 数学优化问题的求解 一般的优化问题不容易求解：计算时间长，不能总是找到解等因素； 有一些问题容易求解： 最小二乘 线性规划 凸优化 3. 最小二乘问题 $min ||Ax-b||_2^2$ 有解析解； 可靠有效的算法； 时间复杂度与$A^{m*n}$相关，为$n^2m$； 容易识别最小二乘问题，可以有一些调整，如正则、权重等； 4. 线性规划 $$min\ c^Tx$$
$$s.t.\ a^Tx\leq b$$
无解析解； 有成熟的算法； 时间复杂度正比于$n^2m(m&amp;gt;n)$ 可以将一些问题转化为线性规划，如$l_1$范数，$l_\infty$范数，分段线性函数问题等； 5. 凸优化问题 定义：目标和约束函数都是凸的；
$f(\alpha x+\beta y)\leq \alpha f(x)+\beta f(y)$
$\alpha + \beta =1, \alpha\geq 0,\beta\geq 0$
计算时间$max\lbrace n^3,n^2m,F\rbrace$，$F$是估计函数$f_i$及其一二阶梯度的代价；
很难识别一个问题是凸优化问题；
很多问题可以转化为凸优化问题，利用成熟的技术求解；
6. 非线性优化 局部最优问题
需要初始值设定 计算快，可拓展到大规模问题 与全局最优无直接关联 全局最优问题：一般利用凸优化解决，但最坏情况可能是指数时间；</description>
    </item>
    
    <item>
      <title>Probability Space</title>
      <link>https://optstats.github.io/2022/08/probability-space/</link>
      <pubDate>Tue, 16 Aug 2022 02:01:58 +0530</pubDate>
      
      <guid>https://optstats.github.io/2022/08/probability-space/</guid>
      <description>概率空间/三元组（$\Omega$,$\mathcal{F}$,$\mathcal{P}$） 样本空间：一个非空的任意集合$\Omega\neq \emptyset$，一个实验的所有可能的输出构成的集合； 事件空间：$\sigma$代数，事件是样本空间中一些样本的集合； 概率测度：将事件空间中的事件映射到$[0,1]$； $\sigma$代数 令$\Omega\neq \emptyset$，$\mathbf{P}(\Omega)$是其幂集，如果$\mathcal{F}\subseteq\mathbf{P}(\Omega)$且满足以下性质：
$\mathcal{F}$包含$\Omega$，即$\Omega \in \mathcal{F}$； $\mathcal{F}$在补运算下是封闭的，即如果$A\in \mathcal{F}$，则$A^c:=\lbrace\Omega\backslash A\rbrace\in \mathcal{F}$； $\mathcal{F}$在可数个并操作下是封闭的，即$A_i\in \mathcal{F}, i=1,2,&amp;hellip;,$则$\bigcup_{i=1}^{\infty}A_i\in \mathcal{F}$； 推论 如果$\mathcal{F}$是一个$\sigma$代数，则其对可数个交操作是封闭的； 假定$\Omega\neq\emptyset$，常见的平凡$\sigma$代数有 $\mathcal{F}=\lbrace\Omega,\emptyset\rbrace$； $\mathcal{F}=\mathbf{P}(\Omega)$； $A\subseteq\Omega$，则$F=\lbrace A,A^c,\emptyset,\Omega\rbrace$是一个$\sigma$代数； $\sigma$代数的生成： 至多可数集合$\Omega$上，可通过可数个集合的划分生成 唯一最小$\sigma$代数 $Borel-\sigma$代数 关于$\Omega$开集的$\sigma$代数，不是由$\Omega$的划分生成；
概率测度 $\Omega\neq\emptyset$ $\mathcal{F}\subseteq\mathcal{P}(\Omega)$为一个$\sigma$代数：一些集合的并，概率空间中表现为事件； 函数$P:\mathcal{F}\rightarrow[0,1]$是一个概率测度当其满足： $P(\Omega)=1$ $P$符合$\sigma$可加性（两两不相交的并）：$P(\bigcup^{\infty}_{i=1}A_i)=\sum_i^{\infty} p(A_i)$ 测度 $X\neq\emptyset$ $\mathcal{F}\subseteq\mathcal{P}(\Omega)$为一个$\sigma$代数 函数$\mu:\mathcal{F}\rightarrow[0,\infty]$是一个测度当其满足： $\mu(\empty)=0$ $\mu$符合$\sigma$可加性（两两不相交的并，也称可列(数)可加性）$\lbrace A_i\rbrace_{i=1}^{\infty}\subseteq\mathcal{F}$： $$\mu(\bigcup^{\infty}_{i=1}A_i)=\sum_i^{\infty} u(A_i)$$ 支撑 定义 一个随机变量$X$的支撑$S_X\in B$是最小的闭集，该闭集上的概率$P(S_X)$为1； 拓扑空间上的开球$B(x,r)$，在开球上$P(B(x,r))\geq 0$； 离散型随机变量的支撑：$\lbrace x\in dom(X)|p(X=x)\geq 0\rbrace$； 理解 测度是对集合“长度”的一种度量，将其映射到一个实值 $\sigma$代数是一个集合的集合，在概率空间上，其表现为事件集合 拉普拉斯概率：有限样本集合上的古典概型 狄拉克测度 随机变量：是一个样本空间到实数上映射 </description>
    </item>
    
  </channel>
</rss>
